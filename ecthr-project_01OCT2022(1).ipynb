{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This project is for processing degal documets downloaded from https://github.com/coastalcph/lex-glue. The data is a collection of case notes and the purpose of this project is to classify each one of them labels indicating which of The European Convention on Human Rights (ECHR) article has been violated in the case described in the note. Although there are officialy 18 articles, only 11 labels are considered in this project.\n",
    "\n",
    "Article 1 – obligation to respect human rights\n",
    "Article 2 – right to life\n",
    "Article 3 – prohibition of torture and cruel, inhuman and degrading treatment\n",
    "Article 4 – prohibition of slavery and forced labour\n",
    "Article 5 – right to liberty and security\n",
    "Article 6 – right to a fair trial\n",
    "Article 7 – no punishment without law\n",
    "Article 8 – right to respect privacy and family life\n",
    "Article 9 – freedom of thought, conscience and religion\n",
    "Article 10 – freedom of expression\n",
    "Article 11 – freedom of assembly and association\n",
    "Article 12 – right to marry\n",
    "Article 13 – right to an effective remedy\n",
    "Article 14 – prohibition of discrimination\n",
    "Article 15 – derogation in time of emergency\n",
    "Article 16 – restriction on political activity of non-nationals\n",
    "Article 17 – prohibition of abuse of rights\n",
    "Article 18 – limitation on use of restriction of rights\n",
    "\n",
    "Each case note contains several sentences and may run into as many as 12000 words. Also, a note may fall under more than one label.\n",
    "\n",
    "The required data is downloaded with the load_dataset python library, which makes it possible to get separate training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lex_glue (C:\\Users\\hustl\\.cache\\huggingface\\datasets\\lex_glue\\ecthr_a\\1.0.0\\c3c0bd7433b636dc39ae49a84dc401190c73156617efc415b04e9835a93a7043)\n",
      "Reusing dataset lex_glue (C:\\Users\\hustl\\.cache\\huggingface\\datasets\\lex_glue\\ecthr_a\\1.0.0\\c3c0bd7433b636dc39ae49a84dc401190c73156617efc415b04e9835a93a7043)\n",
      "Reusing dataset lex_glue (C:\\Users\\hustl\\.cache\\huggingface\\datasets\\lex_glue\\ecthr_a\\1.0.0\\c3c0bd7433b636dc39ae49a84dc401190c73156617efc415b04e9835a93a7043)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset(\"lex_glue\", \"ecthr_a\", split=\"train\")\n",
    "dataset_validation = load_dataset(\"lex_glue\", \"ecthr_a\", split=\"validation\")\n",
    "dataset_test = load_dataset(\"lex_glue\", \"ecthr_a\", split=\"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The data is in the form: \n",
    "[ {text:[sentence, sentence...],labels: [int,int...]}\n",
    " ,{text:[sentence, sentence...],labels: [int,int...]}\n",
    " ,{text:[sentence, sentence...],labels: [int,int...]}\n",
    " ...\n",
    " ...\n",
    "{text:[sentence, sentence...],labels: [int,int...]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=ClassLabel(num_classes=10, names=['2', '3', '5', '6', '8', '9', '10', '11', '14', 'P1-1'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get feature information...\n",
    "dataset_train.info.features\n",
    "#dataset_train.info"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Upon inspecting the data we find that the actual labels are enclosed in the labels property for each od the entries.\n",
    "e.g. labels: [4]  or labels: [1,2] or labels: [0] or labels: [] etc.\n",
    "\n",
    "This assignement is yet to be mapped to the actuall article e.g. Article 1, or Article 3. \n",
    "\n",
    "We will not worry about this mapping for now, however we will consider the labels in the following form \n",
    "No-label, label-0, label-1, label-2...label-10\n",
    "\n",
    "We notice that the text in each entry consistes of an array of sentences. We will convert this into a single string.\n",
    "e.g. {text:[\"aaa\",\"bbb\",\"ccc\"]} will become {text:[\"aaa bbb ccc\"]}\n",
    "\n",
    "Since thaere are cases where the entries are assigned more than one lable e.g. [1,2] we will restructure the data so that each entry will have a single label.\n",
    "\n",
    "e.g.  {text:[\"aaa bbb ccc\"],labels:[1,2]} will become \n",
    " {text:[\"aaa bbb ccc\"],labels:[1]} and  {text:[\"aaa bbb ccc\"],labels:[2]}\n",
    "\n",
    "The processing is carried out in the procedure reprocess_data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess_data(raw_dataset,dataset_type):\n",
    "#find all the sets where there are more than 1 labels\n",
    "#and reprocess them into the origianl dataset, do that\n",
    "#only records with single lables ecists\n",
    "    redefined_dataset=[]\n",
    "    fileIndex=0\n",
    "    for x in raw_dataset:\n",
    "        if(len(x[\"labels\"])==1):            \n",
    "            redefined_dataset.append({\"text\":\" \".join(x[\"text\"]).encode(\"ascii\", \"ignore\").decode(),\"labels\":\"LABEL-\" + str(x[\"labels\"][0])})\n",
    "            fileIndex=fileIndex+1\n",
    "            \n",
    "        if(len(x[\"labels\"])==0):\n",
    "            redefined_dataset.append({\"text\":\" \".join(x[\"text\"]).encode(\"ascii\", \"ignore\").decode(),\"labels\":\"No-LABEL\"})\n",
    "            fileIndex=fileIndex+1\n",
    "            \n",
    "        if(len(x[\"labels\"])>1):\n",
    "            for y in x[\"labels\"]:               \n",
    "                redefined_dataset.append({\"text\":\" \".join(x[\"text\"]).encode(\"ascii\", \"ignore\").decode(),\"labels\":\"LABEL-\" + str(y)})\n",
    "                fileIndex=fileIndex+1\n",
    "        \n",
    "    return redefined_dataset\n",
    "\n",
    "redefined_dataset_train= reprocess_data(dataset_train,\"train\")\n",
    "redefined_dataset_validation= reprocess_data(dataset_validation,\"validation\")\n",
    "redefined_dataset_test= reprocess_data(dataset_test,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL-3\n"
     ]
    }
   ],
   "source": [
    "print(redefined_dataset_train[2][\"labels\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next we convert the data to pandas dataframe objects which are compatible with sklearn and tensorflow which we will be using to build the training models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.  At the beginning of the events relevant t...</td>\n",
       "      <td>LABEL-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.  The applicant is the monarch of Liechtenst...</td>\n",
       "      <td>No-LABEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.  In June 1949 plots of agricultural land ow...</td>\n",
       "      <td>LABEL-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.  In 1991 Mr Duan Slobodnk, a research worke...</td>\n",
       "      <td>LABEL-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.  The applicant is an Italian citizen, born ...</td>\n",
       "      <td>No-LABEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.  In 1987 the applicant association publish...</td>\n",
       "      <td>LABEL-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.  In 1987 the applicant association publish...</td>\n",
       "      <td>LABEL-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.  The applicants are former members of the T...</td>\n",
       "      <td>LABEL-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.  The circumstances in which the applicants ...</td>\n",
       "      <td>LABEL-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.  The circumstances in which the applicants ...</td>\n",
       "      <td>LABEL-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.  On 29 April 1962 the applicant married Mr...</td>\n",
       "      <td>LABEL-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.  The applicant is a Lithuanian national, bo...</td>\n",
       "      <td>LABEL-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.  The applicant is a Lithuanian national, bo...</td>\n",
       "      <td>LABEL-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.  On 11 February 1980, the applicant pleaded...</td>\n",
       "      <td>LABEL-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    labels\n",
       "0   11.  At the beginning of the events relevant t...   LABEL-4\n",
       "1   9.  The applicant is the monarch of Liechtenst...  No-LABEL\n",
       "2   9.  In June 1949 plots of agricultural land ow...   LABEL-3\n",
       "3   8.  In 1991 Mr Duan Slobodnk, a research worke...   LABEL-6\n",
       "4   9.  The applicant is an Italian citizen, born ...  No-LABEL\n",
       "5   12.  In 1987 the applicant association publish...   LABEL-6\n",
       "6   12.  In 1987 the applicant association publish...   LABEL-3\n",
       "7   7.  The applicants are former members of the T...   LABEL-3\n",
       "8   7.  The circumstances in which the applicants ...   LABEL-0\n",
       "9   7.  The circumstances in which the applicants ...   LABEL-2\n",
       "10  11.  On 29 April 1962 the applicant married Mr...   LABEL-3\n",
       "11  7.  The applicant is a Lithuanian national, bo...   LABEL-1\n",
       "12  7.  The applicant is a Lithuanian national, bo...   LABEL-4\n",
       "13  7.  On 11 February 1980, the applicant pleaded...   LABEL-2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(redefined_dataset_train)\n",
    "val_df = pd.DataFrame(redefined_dataset_validation)\n",
    "test_df = pd.DataFrame(redefined_dataset_test)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL-3     4704\n",
       "LABEL-9     1421\n",
       "LABEL-2     1368\n",
       "LABEL-1     1349\n",
       "No-LABEL     914\n",
       "LABEL-4      710\n",
       "LABEL-0      505\n",
       "LABEL-6      291\n",
       "LABEL-8      141\n",
       "LABEL-7      110\n",
       "LABEL-5       41\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we see how may entries we have for each label. The distribution doe not make me very hopefull of a good model because I can see that some of the labels have very few items compared to others. I think a better model will result from having more items per label and having them evenly distributed. I would be happier if each of the labels has about 8000 entries each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11554, 1235, 1243)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists - i.e. get the texts from the datasets\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "val_texts = val_df[\"text\"].tolist()\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "len(train_texts), len(val_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first 3 texts of train_texts\n",
    "#train_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels (\"labels\" columns) and encode them into integers \n",
    "# So that LABEL-0 becomes 0, LABEL-1 1...No-LABEL=10 etc....\n",
    "# So train_labels_encoded will contain an array in the same order as train_texts\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"labels\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"labels\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"labels\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10,  3, ...,  3,  9,  3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what training labels look like\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " array(['LABEL-0', 'LABEL-1', 'LABEL-2', 'LABEL-3', 'LABEL-4', 'LABEL-5',\n",
       "        'LABEL-6', 'LABEL-7', 'LABEL-8', 'LABEL-9', 'No-LABEL'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance \n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLEARN MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use sklern to build a simple model using a pipeline that contains TfidfVectorizer and MultinomialNB\n",
    "\n",
    "#TfidfVectorizer -- Term Frequency – Inverse Document vectorizer which converts the text to numbers that can be computed\n",
    "\n",
    "#MultinomialNB -- The Multinomial Naive Bayes algorithm is a Bayesian learning approach popular in Natural Language Processing (NLP). \n",
    "#The program guesses the tag of a text, such as an email or a newspaper story, using the Bayes theorem. \n",
    "#It calculates each tag's likelihood for a given sample and outputs the tag with the greatest chance.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_texts, \n",
    "            y=train_labels_encoded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2493927125506073"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=val_texts,\n",
    "              y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions  -- model_0.predict() takes an array of texts and returns an array of labels\n",
    "\n",
    "baseline_preds = model_0.predict(val_texts)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('helper_functions.py', <http.client.HTTPMessage at 0x28a434c32e0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "# Get a function for calculating results from the baseline predictions from the validation data\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py'\n",
    "filename = 'helper_functions.py'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import calculate_results helper function\n",
    "from helper_functions import calculate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hustl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 24.93927125506073,\n",
       " 'precision': 0.21566771923225403,\n",
       " 'recall': 0.2493927125506073,\n",
       " 'f1': 0.10788775434330616}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TENSORFLOW MODDELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1795.4989613986497"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long is each text on average?\n",
    "text_lens = [len(text.split()) for text in train_texts]\n",
    "avg_text_len = np.mean(text_lens)\n",
    "avg_text_len # return average sentence length (in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What does the distribution look like?\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(text_lens, bins=100);\n",
    "\n",
    "#Most of the texts have almost 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5813"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(text_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35395"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum sentence length in the training set\n",
    "max(text_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in our vocabulary? \n",
    "max_tokens = 120000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What does the TensorFlow TextVectorization do?\n",
    "\n",
    "It transforms a batch of strings (one example = one string) into either a list of token indices (one example = 1D tensor of integer token indices) or a dense representation (one example = 1D tensor of float values representing data about the example's tokens).\n",
    "\n",
    "TextVectorization is responsible for may things e.g. - tokenization, vocabulary processing,.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text vectorizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens,       # number of words in vocabulary\n",
    "                                    output_sequence_length=35416) # desired output length of vectorized sequences \n",
    "                                                                 # number of words per text\n",
    "                                                                 # we could try 35416, being the max text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training texts\n",
    "text_vectorizer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "4.  The applicant was born in 1963 and lives in Warsaw. 5.  On 18 December 2003 the applicant was arrested by the police on suspicion of having committed assault causing death and dealing in stolen goods. 6.  On 19 December 2003 the Warsaw District Court (Sd Rejonowy) remanded him in custody. It relied on a reasonable suspicion that the applicant had committed the offences with which he had been charged and on the risk that he might induce witnesses to give false testimony. Further, the court referred to the likelihood of a severe prison sentence being imposed on the applicant. 7.  In the course of investigation, the applicants detention was extended by the Ostroka Regional Court (Sd Okrgowy) on 4 March 2004, 11 May 2004 and 10 December 2004. In its decisions, the court relied on the likelihood that the applicant had committed the offence while acting with numerous co-suspects in the ambit of the activities of an organised criminal group. Further, it referred to the need to obtain evidence, in particular: to take evidence from numerous witnesses and suspects and to confront a key witness with the injured persons. 8.  On 29 September 2004 the applicant was convicted and sentenced to one year and four months imprisonment by the Wyszkw District Court for failure to pay maintenance and threatening behaviour, charges which has been examined in separate sets of proceedings. The court offset the sentence against the period of pre-trial detention. 9.  On 23 June 2005 the applicant and twenty three suspects were indicted before the Ostroka Regional Court. The bill of indictment against the applicant comprised charges of assault causing death and dealing in stolen goods, committed in the ambit of the activities of an organised criminal group. However, the applicant was not charged with being a member of such a group. 10.  The applicants detention was extended by the Ostroka Regional Court on 9 May 2005, 8 December 2005, 9 June 2006, 11 September 2006, 11 June 2007 and on 17 October 2007. The court repeated the grounds given in previous decisions. The court also stated that the reasons for the applicants detention were still valid and relied on the likelihood that a severe prison sentence would be imposed on him. 11.  The applicant appealed against decisions extending his detention. The appeals were dismissed on 1 July 2005, 20 January 2006, 11 July 2006 and 20 October 2006. 12.  On 1 February 2006 the court ordered that the cases of 8 of the accused be dealt with in separate proceedings. 13.  On 16 March 2006 and 23 March 2006 the cases of two of the coaccused were earmarked for separate proceedings. 14.  In the meantime, on 9 September 2005, on 27 March 2006 and on 19 March 2007 the applicant lodged unsuccessful applications for release. The court stated that the reasons for his detention were still valid. 15.  Between 13 March 2006 and 4 June 2007 seventy-one hearings took place. 16.  On 11 June 2007 the Ostroka Regional Court convicted the applicant as charged and sentenced him to seven years imprisonment. The applicant appealed. 17.  The case is pending before the Warsaw Appellate Court (Sd Apelacyjny). 18.  The applicant failed to lodge a complaint about a breach of the right to a trial within a reasonable time with the domestic court, under section 5 of the Law of 17 June 2004 (Ustawa o skardze na naruszenie prawa strony do rozpoznania sprawy w postpowaniu sdowym bez nieuzasadnionej zwoki) (the 2004 Act).\n",
      "\n",
      "Length of text: 583\n",
      "\n",
      "Vectorized text:\n",
      "[[133   2  12 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Test out text vectorizer\n",
    "import random\n",
    "target_text = random.choice(train_texts)\n",
    "print(f\"Text:\\n{target_text}\")\n",
    "print(f\"\\nLength of text: {len(target_text.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_text])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 104748\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'of', 'to']\n",
      "Least common words in the vocabulary: ['00076', '0005', '00012', '00003', '0000']\n"
     ]
    }
   ],
   "source": [
    "# How many words in our training vocabulary?\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"), \n",
    "print(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization_2',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 120000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 35416,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Embedding in TensorFlow?\n",
    "\n",
    "An embedding is a dense vector of floating point values (the length of the vector is \n",
    "a parameter you specify). Instead of specifying the values for the embedding manually, \n",
    "they are trainable parameters (weights learned by the model during training, in the \n",
    "same way a model learns weights for a dense layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                               # Use masking to handle variable sequence lengths (save space)\n",
    "                               mask_zero=True,\n",
    "                               name=\"token_embedding\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "4.  The applicant was born in 1963 and lives in Warsaw. 5.  On 18 December 2003 the applicant was arrested by the police on suspicion of having committed assault causing death and dealing in stolen goods. 6.  On 19 December 2003 the Warsaw District Court (Sd Rejonowy) remanded him in custody. It relied on a reasonable suspicion that the applicant had committed the offences with which he had been charged and on the risk that he might induce witnesses to give false testimony. Further, the court referred to the likelihood of a severe prison sentence being imposed on the applicant. 7.  In the course of investigation, the applicants detention was extended by the Ostroka Regional Court (Sd Okrgowy) on 4 March 2004, 11 May 2004 and 10 December 2004. In its decisions, the court relied on the likelihood that the applicant had committed the offence while acting with numerous co-suspects in the ambit of the activities of an organised criminal group. Further, it referred to the need to obtain evidence, in particular: to take evidence from numerous witnesses and suspects and to confront a key witness with the injured persons. 8.  On 29 September 2004 the applicant was convicted and sentenced to one year and four months imprisonment by the Wyszkw District Court for failure to pay maintenance and threatening behaviour, charges which has been examined in separate sets of proceedings. The court offset the sentence against the period of pre-trial detention. 9.  On 23 June 2005 the applicant and twenty three suspects were indicted before the Ostroka Regional Court. The bill of indictment against the applicant comprised charges of assault causing death and dealing in stolen goods, committed in the ambit of the activities of an organised criminal group. However, the applicant was not charged with being a member of such a group. 10.  The applicants detention was extended by the Ostroka Regional Court on 9 May 2005, 8 December 2005, 9 June 2006, 11 September 2006, 11 June 2007 and on 17 October 2007. The court repeated the grounds given in previous decisions. The court also stated that the reasons for the applicants detention were still valid and relied on the likelihood that a severe prison sentence would be imposed on him. 11.  The applicant appealed against decisions extending his detention. The appeals were dismissed on 1 July 2005, 20 January 2006, 11 July 2006 and 20 October 2006. 12.  On 1 February 2006 the court ordered that the cases of 8 of the accused be dealt with in separate proceedings. 13.  On 16 March 2006 and 23 March 2006 the cases of two of the coaccused were earmarked for separate proceedings. 14.  In the meantime, on 9 September 2005, on 27 March 2006 and on 19 March 2007 the applicant lodged unsuccessful applications for release. The court stated that the reasons for his detention were still valid. 15.  Between 13 March 2006 and 4 June 2007 seventy-one hearings took place. 16.  On 11 June 2007 the Ostroka Regional Court convicted the applicant as charged and sentenced him to seven years imprisonment. The applicant appealed. 17.  The case is pending before the Warsaw Appellate Court (Sd Apelacyjny). 18.  The applicant failed to lodge a complaint about a breach of the right to a trial within a reasonable time with the domestic court, under section 5 of the Law of 17 June 2004 (Ustawa o skardze na naruszenie prawa strony do rozpoznania sprawy w postpowaniu sdowym bez nieuzasadnionej zwoki) (the 2004 Act).\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[133   2  12 ...   0   0   0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.0487619  -0.04826738 -0.00083016 ...  0.0242268   0.04395671\n",
      "   -0.0488984 ]\n",
      "  [ 0.01809997 -0.00510689 -0.04090464 ... -0.0449113   0.04186847\n",
      "   -0.02121352]\n",
      "  [ 0.01846978  0.04137529 -0.00548904 ... -0.01147803 -0.017413\n",
      "   -0.00601412]\n",
      "  ...\n",
      "  [-0.0084254   0.03640019 -0.00320339 ...  0.02194324 -0.04239389\n",
      "    0.01663843]\n",
      "  [-0.0084254   0.03640019 -0.00320339 ...  0.02194324 -0.04239389\n",
      "    0.01663843]\n",
      "  [-0.0084254   0.03640019 -0.00320339 ...  0.02194324 -0.04239389\n",
      "    0.01663843]]]\n",
      "\n",
      "Embedded sentence shape: (1, 35416, 128)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_text}\\n\")\n",
    "vectorized_text = text_vectorizer([target_text])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_text}\\n\")\n",
    "embedded_text = token_embed(vectorized_text)\n",
    "print(f\"Sentence after embedding:\\n{embedded_text}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5813"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(text_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub USE\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow-estimator==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random training text:\n",
      "5.  On 13 June 2007 the Ust-Ilimsk Town Court of the Irkutsk Region found the applicant guilty of inflicting grave bodily harm which caused the death of the victim and sentenced him to a prison term. The applicant appealed, claiming in general terms that the proceedings had been unfair, and that the court had applied domestic law and assessed the evidence erroneously. He further alleged that the court had failed to examine two witnesses and had delivered its judgment in the absence of one of the codefendants. 6.  On 1 October 2007 the Irkutsk Regional Court quashed the conviction on appeal and remitted the case to the first-instance court for a fresh examination. 7.  On 20 March 2009 the Ust-Ilimsk Town Court found the applicant guilty of the same offence and gave him a custodial sentence. The applicant appealed. Giving no further details, he alleged that the Town Court had applied domestic law and assessed the evidence erroneously, and that it had rejected the majority of the motions lodged by the defence. He further complained about the change of his counsel during the proceedings. 8.  On 5 October 2009 the Irkutsk Regional Court rejected the applicants complaint and upheld the judgment on appeal. 9.  Between 26 September 2005 and 9 January 2009 the applicant was detained in remand prison IZ-38/2 of the Irkutsk Region. The prison was severely overcrowded. Thus, cell 95 measuring 24 sq. m was equipped with 8 sleeping places and accommodated up to 17 inmates. In addition, the applicant claimed that he had contracted tuberculosis during his stay there. 10.  From 23 February to 27 April 2008 the applicant was transferred to remand prison IZ-38/1 of Irkutsk and from 17 July to 3 August 2008 he was transferred to penitentiary medical facility LIU-27 in the Irkutsk Region for treatment of his tuberculosis.\n",
      "\n",
      "Text after embedding:\n",
      "[-0.06157638 -0.00339778 -0.06084884 -0.04795378  0.05703561  0.03406185\n",
      "  0.03960619 -0.03674253  0.01873541 -0.06222438 -0.04198517  0.01402738\n",
      "  0.05858345 -0.04116859  0.02518352  0.0361952  -0.05546245  0.02907356\n",
      " -0.06011331  0.05889612 -0.01184994 -0.05664955  0.05133652 -0.05103428\n",
      " -0.05903365  0.04955348 -0.04530388 -0.02518971  0.05489806 -0.00562847] (truncated output)...\n",
      "\n",
      "Length of text embedding:\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# Test out the embedding on a random text\n",
    "random_training_text = random.choice(train_texts)\n",
    "print(f\"Random training text:\\n{random_training_text}\\n\")\n",
    "use_embedded_text = tf_hub_embedding_layer([random_training_text])\n",
    "print(f\"Text after embedding:\\n{use_embedded_text[0][:30]} (truncated output)...\\n\")\n",
    "print(f\"Length of text embedding:\\n{len(use_embedded_text[0])}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Labels need to be one hot encoded before we can turn them into y input for the tensorFlow model\n",
    "e.g  LABEL-0 becomes [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] \n",
    "and No-LABEL becomes [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"labels\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"labels\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"labels\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what training labels look like\n",
    "train_labels_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(11,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_texts, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_texts, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is \"experimental.AUTOTUNE\" ?\n",
    "tf.data.experimental.AUTOTUNE defines appropriate number of processes that are free for working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 11), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.  At the beginning of the events relevant t...</td>\n",
       "      <td>LABEL-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.  The applicant is the monarch of Liechtenst...</td>\n",
       "      <td>No-LABEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.  In June 1949 plots of agricultural land ow...</td>\n",
       "      <td>LABEL-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.  In 1991 Mr Duan Slobodnk, a research worke...</td>\n",
       "      <td>LABEL-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.  The applicant is an Italian citizen, born ...</td>\n",
       "      <td>No-LABEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  11.  At the beginning of the events relevant t...   LABEL-4\n",
       "1  9.  The applicant is the monarch of Liechtenst...  No-LABEL\n",
       "2  9.  In June 1949 plots of agricultural land ow...   LABEL-3\n",
       "3  8.  In 1991 Mr Duan Slobodnk, a research worke...   LABEL-6\n",
       "4  9.  The applicant is an Italian citizen, born ...  No-LABEL"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect training dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token inputs\n",
    "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(7, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Combine token and char embeddings into a hybrid embedding\n",
    "combined_embeddings = token_model.output\n",
    "z = layers.Dense(64, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "z = layers.Dense(128, activation=\"relu\")(z)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "#z = layers.Dense(64, activation=\"relu\")(z)\n",
    "#z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    \n",
    "# 3. Create output layer\n",
    "output_layer = layers.Dense(11, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "\n",
    "# 4. Put together model\n",
    "model_5 = tf.keras.Model(inputs=[#line_number_model.input,\n",
    "                                 #total_line_model.input,\n",
    "                                 token_model.input,\n",
    "                                 ],\n",
    "                                 outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " token_inputs (InputLayer)   [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                512       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 11)                1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,811,666\n",
      "Trainable params: 13,842\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of our token, char and positional embedding model\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x0000028A58928C70> True\n",
      "<tensorflow_hub.keras_layer.KerasLayer object at 0x0000028A6EA90DC0> False\n",
      "<keras.layers.core.dense.Dense object at 0x0000028A588E36D0> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000028A390E03A0> True\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x0000028A6E8C6F70> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000028A4BC96910> True\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x0000028A67AC6850> True\n",
      "<keras.layers.core.dense.Dense object at 0x0000028A390E0550> True\n"
     ]
    }
   ],
   "source": [
    "# Check which layers of our model are trainable or not\n",
    "for layer in model_5.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token, char, positional embedding model\n",
    "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # add label smoothing (examples which are really confident get smoothed a little)\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 11), dtype=tf.float64, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 11), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation datasets (all four kinds of inputs)\n",
    "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((#train_line_numbers_one_hot, # line numbers\n",
    "                                                                #train_total_lines_one_hot, # total lines\n",
    "                                                                train_texts#, # train tokens\n",
    "                                                                #train_chars\n",
    "                                                                )) # train chars\n",
    "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels\n",
    "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels\n",
    "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE) # turn into batches and prefetch appropriately\n",
    "\n",
    "# Validation dataset\n",
    "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((#val_line_numbers_one_hot,\n",
    "                                                              #val_total_lines_one_hot,\n",
    "                                                              val_texts#,\n",
    "                                                              #val_chars\n",
    "                                                              ))\n",
    "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
    "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE) # turn into batches and prefetch appropriately\n",
    "\n",
    "# Check input shapes\n",
    "train_pos_char_token_dataset, val_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "36/36 [==============================] - 11s 307ms/step - loss: 1.8715 - accuracy: 0.4948 - val_loss: 1.9608 - val_accuracy: 0.3958\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 10s 271ms/step - loss: 1.7210 - accuracy: 0.5590 - val_loss: 1.9516 - val_accuracy: 0.3750\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 8s 229ms/step - loss: 1.6961 - accuracy: 0.5434 - val_loss: 1.9557 - val_accuracy: 0.3438\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 9s 243ms/step - loss: 1.6932 - accuracy: 0.5347 - val_loss: 1.9400 - val_accuracy: 0.3646\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 10s 288ms/step - loss: 1.7809 - accuracy: 0.4766 - val_loss: 1.9373 - val_accuracy: 0.3646\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 10s 281ms/step - loss: 1.7242 - accuracy: 0.5234 - val_loss: 1.9348 - val_accuracy: 0.4062\n",
      "Epoch 7/15\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 1.7910 - accuracy: 0.4991 - val_loss: 1.9209 - val_accuracy: 0.3854\n",
      "Epoch 8/15\n",
      "36/36 [==============================] - 14s 372ms/step - loss: 1.8569 - accuracy: 0.4384 - val_loss: 1.8940 - val_accuracy: 0.3958\n",
      "Epoch 9/15\n",
      "36/36 [==============================] - 14s 396ms/step - loss: 1.8565 - accuracy: 0.4557 - val_loss: 1.8829 - val_accuracy: 0.4062\n",
      "Epoch 10/15\n",
      "36/36 [==============================] - 13s 378ms/step - loss: 1.8837 - accuracy: 0.4149 - val_loss: 1.8619 - val_accuracy: 0.4271\n",
      "Epoch 11/15\n",
      " 1/36 [..............................] - ETA: 11s - loss: 1.8075 - accuracy: 0.4375WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 540 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 540 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.7838 - accuracy: 0.4412 - val_loss: 1.8600 - val_accuracy: 0.4271\n"
     ]
    }
   ],
   "source": [
    "# Fit the token, char and positional embedding model\n",
    "history_model_5 = model_5.fit(train_pos_char_token_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
    "                              epochs=15,\n",
    "                              validation_data=val_pos_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_pos_char_token_dataset))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model5_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model5_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save best performing model to SavedModel format (default)\n",
    "model_5.save(\"model5_model\") # model will be saved to path specified by string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
